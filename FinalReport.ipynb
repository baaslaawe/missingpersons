{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Building Find Us Missing Person API Using Open Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How many people go missing every day in the United States? The answer to this question should be a simple statistic, but there are many federally and privately funded missing persons databases in the United States. Though all designed to assist investigators and the general public, these databases are disconnected, individual entities with information is neither centralized nor standardized. \n",
      "\n",
      "Also none of the missing person databases provide an application programming interface (API). APIs are commonly provided to enable programmatic data access by researchers and application developers alike."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Project Proposal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Originally I proposed to scrap <a href=\"http://www.findthemissing.org\">National Missing and Unidentified Persons System (NamUs)</a> using <a href=\"http://scrapy.org\">Scrapy</a> and host the data on <a href=\"http://morph.io\">Morph</a>, which automatically provides an API. However, because NamUs populates its webpage with AJAX, Scrapy is not able to scrape the pages correctly, because the AJAX calls are not triggered. I tried several alternative tools, but ended up using <a href=\"http://docs.seleniumhq.org\">Selenium</a> for browser automation. Because Morph is only designed to host Scrapy scrappers, I decided use <a href=\"https://www.heroku.com\">Heroku</a> to <a href=\"http://www.postgresql.org\">PostgreSQL</a> databases and use <a href=\"http://flask.pocoo.org\">Flask</a> to handle the API requests.\n",
      "\n",
      "I also considered incorporating another federal database into this project, but decided to work with <a href=\"http://www.missingkids.com\">The National Center for Missing & Exploited Children (NCMEC)</a>, a private non-profit organization. Monitoring network traffic while executing manual searches on NCMEC's website exposed an API the website uses to populate its webpages. I used NCMEC API to extract information about its missing children's cases."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To run my code, you will need to clone my project from github:\n",
      "<br><br><a href=\"https://github.com/jcmack/missingpersons\">https://github.com/jcmack/missingpersons</a>\n",
      "\n",
      "Then run:\n",
      "<br><br><code>pip install requirements.txt</code>\n",
      "\n",
      "Also you will have to download and install <a href=\"http://www.postgresql.org\">PostgreSQL</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "NCMEC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a <a href=\"http://nbviewer.ipython.org/gist/anonymous/4357ae2ea7bd1ffb3894\">tutorial notebook on extracting data from NCMEC</a>. \n",
      "\n",
      "Running <a href=\"https://github.com/jcmack/missingpersons/blob/master/ncmec.py\">ncmec.py</a> from the project source code extracts NCMEC missing persons cases and creates a new <a href=\"https://github.com/jcmack/missingpersons/blob/master/data/ncmec_ca.json\">ncmec_ca.json</a> file. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "NamUs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a <a href=\"http://nbviewer.ipython.org/gist/anonymous/8de2535ea80dfc35c228\">tutorial notebook on extracting data from NamUs</a>. \n",
      "\n",
      "Running <a href=\"https://github.com/jcmack/missingpersons/blob/master/namus.py\">namus.py</a> from the project source code extracts NamUs missing persons cases and creates a new <a href=\"https://github.com/jcmack/missingpersons/blob/master/data/merged_ca.json\">merged_ca.json</a> file. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cleaning Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because missing persons cases do not have a standardizations, the data from NCMEC and NamUs had to be cleaned and grouped together. I created common.py and a set of <code>clean_</code> functions that force the data to conform to the same standards. These functions are invoked when the data is extracted thus already in ncmec.py and namus.py."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Capitalizations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NCMEC had some of the missing person's information in ALL CAPS while Namus used traditional capitalizations. Thus I wrote a function that capitalizes the first letter of a name. For the name contains multiple words then each word is capitalized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def capitalize(s):\n",
      "\tif s:\n",
      "\t\tarr = s.strip().split(\" \")\n",
      "\t\trets = \"\"\n",
      "\t\tfor subs in arr:\n",
      "\t\t\trets += subs[0].upper() + subs[1:].lower() + \" \"\n",
      "\t\treturn rets.strip()\n",
      "\treturn \"\"\n",
      "print capitalize(\"MARY LOUISE\")\n",
      "print capitalize(\"JACKSON\")\n",
      "print capitalize(\"christopher\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mary Louise\n",
        "Jackson\n",
        "Christopher\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Race"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Race information between the two databases were very different. NCMEC had white, black, whitehisp, biracial, pacific, asian, hispanic, amind and multiracial, while NamUs had White, Black/African American, Asian or Pacific Islander, White Hispanic/Latino, Non-White Hispanic/Latino, Native American, Other or Unknown.\n",
      "\n",
      "NCMEC's multiracial category had to be re-classified as Other, because NamUs does not have a specific multiracial category. NCMEC's pacific and asian were both place into the Asian or Pacific Islander category. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_race(race):\n",
      "\traces = [\"White\", \"Black/African American\", \"Asian or Pacific Islander\", \"Native American\", \"Non-White Hispanic/Latino\", \"White Hispanic/Latino\", \"Other\", \"Unknown\"]\n",
      "\tif race in races:\n",
      "\t\treturn race\n",
      "\telif race.lower() == \"white\":\n",
      "\t\treturn \"White\"\n",
      "\telif race.lower() == \"black\":\n",
      "\t\treturn \"Black/African American\"\n",
      "\telif race.lower() == \"whitehisp\":\n",
      "\t\treturn \"White Hispanic/Latino\"\n",
      "\telif race.lower() == \"biracial\":\n",
      "\t\treturn \"Other\"\n",
      "\telif race.lower() == \"pacific\" or race.lower() == \"asian\":\n",
      "\t\treturn \"Asian or Pacific Islander\"\n",
      "\telif race.lower() == \"hispanic\":\n",
      "\t\treturn \"Non-White Hispanic/Latino\"\n",
      "\telif race.lower() == \"amind\":\n",
      "\t\treturn \"Native American\"\n",
      "\treturn \"Unknown\"\n",
      "\n",
      "print clean_race(\"black\")\n",
      "print clean_race(\"pacific\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Black/African American\n",
        "Asian or Pacific Islander\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Hair Color"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both NCMEC and NamUs support a wide range of hair colors, but I elected to use standard hair colors that the U.S. supports for driver licenses and Unknown:\n",
      "\n",
      "* Black\n",
      "\n",
      "* Blonde\n",
      "\n",
      "* Brown\n",
      "\n",
      "* Gray\n",
      "\n",
      "* Red/Auburn\n",
      "\n",
      "* Sandy\n",
      "\n",
      "* White\n",
      "\n",
      "* Unknown"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_hair_color(hair_color):\n",
      "\thair_colors = [\"Brown\", \"Sandy\", \"Black\", \"Gray\", \"White\", \"Blonde\", \"Red/Auburn\"]\n",
      "\tif hair_color in hair_colors:\n",
      "\t\treturn hair_color\n",
      "\telif capitalize(hair_color) in hair_colors:\n",
      "\t\treturn capitalize(hair_color)\n",
      "\telif hair_color.lower() == \"gray or partially gray\" or hair_color.lower() == \"grey\":\n",
      "\t\treturn \"Gray\"\n",
      "\telif hair_color.lower() == \"red\" or hair_color.lower() == \"auburn\":\n",
      "\t\treturn \"Red/Auburn\"\n",
      "\telif hair_color.lower() == \"ltbrown\":\n",
      "\t\treturn \"Brown\"\n",
      "\telif hair_color.lower() == \"blond/strawberry\":\n",
      "\t\treturn \"Blonde\"\n",
      "\treturn \"Unknown\"\n",
      "\n",
      "print clean_hair_color(\"ltbrown\")\n",
      "print clean_hair_color(\"white\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Brown\n",
        "White\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Building an API"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Populating the Database"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After I gathered, cleaned and merged all of the data from NCMEC and NamUs into <a href=\"https://raw.githubusercontent.com/jcmack/missingpersons/master/data/merged_ca.json\">merged_ca.json</a>, I created a new local PostgreSQL database called \"find-us-db\".\n",
      "\n",
      "I then create a table in \"find-us-db\" and upload the information using <a href=\"https://github.com/jcmack/missingpersons/blob/master/db.py\">db.py</a>. Modify this line in db.py accordingly based on your credentionals:\n",
      "\n",
      "<code>conn_string = \"host='localhost' dbname='find-us-db' user='postgres' password='secret'\"</code>\n",
      "\n",
      "Then export the local database and import into heroku's database. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Handling API Requests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I used <a href=\"http://flask.pocoo.org\">Flask</a> to build a basic front end to handle requests and <a href=\"http://initd.org/psycopg/\">Psycopg2</a> to execute PostgreSQL commands on the database. I built the Flask front end expanding on this <a href=\"http://virantha.com/2013/11/14/starting-a-simple-flask-app-with-heroku\">tutorial</a>, which also covers how to deploy a Heroku Flask project.\n",
      "\n",
      "The source for the Heroku application is in the <a href=\"https://github.com/jcmack/missingpersons/tree/master/app\">app</a> folder of my github project. The actual application is deployed at: http://find-us.herokuapp.com"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a <a href=\"http://nbviewer.ipython.org/gist/anonymous/05139e3fa807aeab74e3\">tutorial notebook on working with Find Us: Missing Persons API</a> as well <a href=\"https://github.com/jcmack/missingpersons\">complete documentation on github</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Future Work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Re-implement NamUs data extraction. The current implementation of data extraction from NamUs using Selenium takes too long to complete and is not feasible.\n",
      "\n",
      "* Contact missing persons organizations to provide a common API.\n",
      "\n",
      "* Expand database to include more missing persons databases. \n",
      "\n",
      "* Implement better ways of removing duplicate missing persons cases.\n",
      "\n",
      "* Auto-categorize missing persons cases such as runaways, family abductions, etc.\n",
      "\n",
      "* Implement automatic scheduled scraping to populate the database with the latest cases.\n",
      "\n",
      "* Provide database snapshots for researchers trying to determine trends with missing persons cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Division of Labor"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I worked all of portions of this project individually. I received feedback on my project from my classmates during the notebook review and direction from my instructor."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}